

# YOLOv3
## Bounding Box
<p align="center"><img src="https://github.com/em-1001/YOLOv3/blob/master/image/bbox.png"></p>


YOLOv2 ë¶€í„° Anchor box(prior box)ë¥¼ ë¯¸ë¦¬ ì„¤ì •í•˜ì—¬ ìµœì¢… bounding box ì˜ˆì¸¡ì— í™œìš©í•œë‹¤. ìœ„ ê·¸ë¦¼ì—ì„œëŠ” $b_x, b_y, b_w, b_h$ê°€ ìµœì¢…ì ìœ¼ë¡œ ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” bounding boxì´ë‹¤. ê²€ì€ ì ì„ ì€ ì‚¬ì „ì— ì„¤ì •ëœ Anchor boxë¡œ ì´ Anchor boxë¥¼ ì¡°ì •í•˜ì—¬ íŒŒë€ìƒ‰ì˜ bounding boxë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•œë‹¤.   

ëª¨ë¸ì€ ì§ì ‘ì ìœ¼ë¡œ $b_x, b_y, b_w, b_h$ë¥¼ ì˜ˆì¸¡í•˜ì§€ ì•Šê³  $t_x, t_y, t_w, t_h$ë¥¼ ì˜ˆì¸¡í•˜ê²Œ ëœë‹¤. 
ë²”ìœ„ì œí•œì´ ì—†ëŠ” $t_x, t_y$ì— sigmoid($\sigma$)ë¥¼ ì ìš©í•´ì£¼ì–´ 0ê³¼ 1ì‚¬ì˜ ê°’ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ê³ , ì´ë¥¼ í†µí•´ bboxì˜ ì¤‘ì‹¬ ì¢Œí‘œê°€ 1ì˜ í¬ê¸°ë¥¼ ê°–ëŠ” í˜„ì¬ cellì„ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ í•´ì¤€ë‹¤. ì—¬ê¸°ì— offsetì¸ $c_x, c_y$ë¥¼ ë”í•´ì£¼ë©´ ìµœì¢…ì ì¸ bboxì˜ ì¤‘ì‹¬ ì¢Œí‘œë¥¼ ì–»ê²Œ ëœë‹¤.    
$b_w, b_h$ì˜ ê²½ìš° ë¯¸ë¦¬ ì •í•´ë‘” Anchor boxì˜ ë„ˆë¹„ì™€ ë†’ì´ë¥¼ ì–¼ë§Œí¼ì˜ ë¹„ìœ¨ë¡œ ì¡°ì ˆí•  ì§€ë¥¼ Anchorì™€ $t_w, t_h$ì— ëŒ€í•œ log scaleì„ ì´ìš©í•´ êµ¬í•œë‹¤. 

YOLOv2ì—ì„œëŠ” bboxë¥¼ ì˜ˆì¸¡í•  ë•Œ $t_x, t_y, t_w, t_h$ë¥¼ ì˜ˆì¸¡í•œ í›„ ê·¸ë¦¼ì—ì„œì˜ $b_x, b_y, b_w, b_h$ë¡œ ë³€í˜•í•œ ë’¤ $L_2$ lossë¥¼ í†µí•´ í•™ìŠµì‹œì¼°ì§€ë§Œ, YOLOv3ì—ì„œëŠ” ground truthì˜ ì¢Œí‘œë¥¼ ê±°ê¾¸ë¡œ $\hat{t}_ {âˆ—}$ë¡œ ë³€í˜•ì‹œì¼œ ì˜ˆì¸¡í•œ $t_{âˆ—}$ì™€ ì§ì ‘ $L_1$ lossë¡œ í•™ìŠµì‹œí‚¨ë‹¤. ground truthì˜ $x, y$ì¢Œí‘œì˜ ê²½ìš° ì•„ë˜ì™€ ê°™ì´ ë³€í˜•ë˜ê³ , 

$$
\begin{aligned}
&b_{âˆ—}= \sigma(\hat{t}_ {âˆ—}) + c_{âˆ—}\\      
&\sigma(\hat{t}_ {âˆ—}) = b_{âˆ—} - c_{âˆ—}\\      
&\hat{t}_ {âˆ—} = \sigma^{-1}(b_{âˆ—} - c_{âˆ—})
\end{aligned}$$

$w, h$ëŠ” ì•„ë˜ì™€ ê°™ì´ ë³€í˜•ëœë‹¤. 

$$\hat{t}_ {âˆ—} = \ln\left(\frac{b_{âˆ—}}{p_{âˆ—}}\right)$$

ê²°ê³¼ì ìœ¼ë¡œ $x, y, w, h$ lossëŠ” ground truthì¸ $\hat{t}_ {âˆ—}$ prediction valueì¸ ${t}_ {âˆ—}$ì‚¬ì´ì˜ ì°¨ì´ $\hat{t}_ {âˆ—} - {t}_ {âˆ—}$ë¥¼ í†µí•œ Sum-Squared Error(SSE)ë¡œ êµ¬í•´ì§„ë‹¤. 

## Model

<p align="center"><img src="https://github.com/em-1001/YOLOv3/blob/master/image/darknet53.png" height="35%" width="35%"><img src="https://github.com/em-1001/YOLOv3/blob/master/image/model1.png" height="65%" width="65%"></p>


ëª¨ë¸ì˜ backboneì€ $3 \times 3$, $1 \times 1$ Residual connectionì„ ì‚¬ìš©í•˜ë©´ì„œ ìµœì¢…ì ìœ¼ë¡œ 53ê°œì˜ conv layerë¥¼ ì‚¬ìš©í•˜ëŠ” **Darknet-53** ì„ ì´ìš©í•œë‹¤. Darknet-53ì˜ Residual blockì•ˆì—ì„œë„ bottleneck êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ë©°, inputì˜ channelì„ ì¤‘ê°„ì— ë°˜ìœ¼ë¡œ ì¤„ì˜€ë‹¤ê°€ ë‹¤ì‹œ ë³µêµ¬ì‹œí‚¨ë‹¤. ì´ë•Œ Residual blockì˜ $1 \times 1$ convëŠ” $s=1, p=0$ ì´ê³ , $3 \times 3$ convëŠ” $s=1, p=1$ì´ë‹¤. 

YOLOv3 modelì˜ íŠ¹ì§•ì€ ë¬¼ì²´ì˜ scaleì„ ê³ ë ¤í•˜ì—¬ 3ê°€ì§€ í¬ê¸°ì˜ outputì´ ë‚˜ì˜¤ë„ë¡ FPNê³¼ ìœ ì‚¬í•˜ê²Œ ì„¤ê³„í•˜ì˜€ë‹¤ëŠ” ê²ƒì´ë‹¤. ì˜¤ë¥¸ìª½ ê·¸ë¦¼ê³¼ ê°™ì´ $416 \times 416$ì˜ í¬ê¸°ë¥¼ feature extractorë¡œ ë°›ì•˜ë‹¤ê³  í•˜ë©´, feature mapì´ í¬ê¸°ê°€ $52 \times 52$, $26 \times 26$, $13 \times 13$ì´ ë˜ëŠ” layerì—ì„œ ê°ê° feature mapì„ ì¶”ì¶œí•œë‹¤. 

<img src="https://github.com/em-1001/YOLOv3/blob/master/image/model2.png"> 

ê·¸ ë‹¤ìŒ ê°€ì¥ ë†’ì€ level, ì¦‰ í•´ìƒë„ê°€ ê°€ì¥ ë‚®ì€ feature mapë¶€í„° $1 \times 1$, $3 \times 3$ conv layerë¡œ êµ¬ì„±ëœ ì‘ì€ Fully Convolutional Network(FCN)ì— ì…ë ¥í•œë‹¤. ì´í›„ ì´ FCNì˜ output channelì´ 512ê°€ ë˜ëŠ” ì‹œì ì—ì„œ feature mapì„ ì¶”ì¶œí•œ ë’¤, $2\times$ë¡œ upsamplingì„ ì§„í–‰í•œë‹¤. ì´í›„ ë°”ë¡œ ì•„ë˜ levelì— ìˆëŠ” feature mapê³¼ concatenateë¥¼ í•´ì£¼ê³ , ì´ë ‡ê²Œ ë§Œë“¤ì–´ì§„ merged feature mapì„ ë‹¤ì‹œ FCNì— ì…ë ¥í•œë‹¤. ì´ ê³¼ì •ì„ ë‹¤ìŒ levelì—ë„ ë˜‘ê°™ì´ ì ìš©í•´ì£¼ê³  ì´ë ‡ê²Œ 3ê°œì˜ scaleì„ ê°€ì§„ feature mapì´ ë§Œë“¤ì–´ì§„ë‹¤. ê° scaleì— ë”°ë¼ ë‚˜ì˜¤ëŠ” ìµœì¢… feature mapì˜ í˜•íƒœëŠ” $N \times N \times \left[3 \cdot (4+1+80)\right]$ì´ë‹¤. ì—¬ê¸°ì„œ $3$ì€ grid cellë‹¹ predictí•˜ëŠ” anchor boxì˜ ìˆ˜ë¥¼, $4$ëŠ” bounding box offset $(x, y, w, h)$, $1$ì€ objectness prediction, $80$ì€ classì˜ ìˆ˜ ì´ë‹¤. ë”°ë¼ì„œ ìµœì¢…ì ìœ¼ë¡œ ì–»ëŠ” feature mapì€ $\left[52 \times 52 \times 255\right], \left[26 \times 26 \times 255\right], \left[13 \times 13 \times 255\right]$ì´ë‹¤. 

ì´ëŸ¬í•œ ë°©ë²•ì„ í†µí•´ ë” ë†’ì€ levelì˜ feature mapìœ¼ë¡œë¶€í„° fine-grained ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆìœ¼ë©°, ë” ë‚®ì€ levelì˜ feature mapìœ¼ë¡œë¶€í„° ë” ìœ ìš©í•œ semantic ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.



## Loss Function

$$Î»_ {coord} \sum_ {i=0}^{S^2} \sum_ {j=0}^B ğŸ™^{obj}_ {i j} \left[(t_ {x_ i} - \hat{t_ {x_ i}})^2 + (t_ {y_ i} - \hat{t_ {y_ i}})^2 \right]$$

$$+Î»_ {coord} \sum_ {i=0}^{S^2} \sum_ {j=0}^B ğŸ™^{obj}_ {i j} \left[(t_ {w_ i} - \hat{t_ {w_ i}})^2 + (t_ {h_ i} - \hat{t_ {h_ i}})^2 \right]$$

$$+\sum_{i=0}^{S^2} \sum_{j=0}^B ğŸ™^{obj}_{i j} \left[-(o_i\log(\hat{o_i}) + (1 - o_i)\log(1 - \hat{o_i}))\right]$$

$$+Mask_{ig} \cdot Î»_{noobj} \sum_{i=0}^{S^2} \sum_{j=0}^B ğŸ™^{noobj}_{i j} \left[-(o_i\log(\hat{o_i}) + (1 - o_i)\log(1 - \hat{o_i}))\right]$$

$$+\sum_{i=0}^{S^2} \sum_{j=0}^B ğŸ™^{obj}_ {i j} \sum_{c \in classes} \left[-(c_i\log(\hat{c_i}) + (1 - c_i)\log(1 - \hat{c_i}))\right]$$  

$S$ : number of cells    
$B$ : number of anchors  
$o$ : objectness  
$c$ : class label  
$Î»_ {coord}$ : coordinate loss balance constant  
$Î»_{noobj}$ : no confidence loss balance constant    
$ğŸ™^{obj}_ {i j}$ : 1 when there is object, 0 when there is no object  
$ğŸ™^{noobj}_ {i j}$ : 1 when there is no object, 0 when there is object  
$Mask_{ig}$ : tensor that masks only the anchor with iou $\le$ 0.5. Have a shape of $\left[S, S, B\right]$.

ê°ê°ì˜ boxëŠ” multi-label classificationì„ í•˜ê²Œ ë˜ëŠ”ë° ë…¼ë¬¸ì—ì„œëŠ” softmaxê°€ ì„±ëŠ¥ì´ ì¢‹ì§€ ëª»í•˜ê¸° ë•Œë¬¸ì—, binary cross-entropy lossë¥¼ ì‚¬ìš©í–ˆë‹¤ê³  í•œë‹¤. í•˜ë‚˜ì˜ boxì•ˆì— ë³µìˆ˜ì˜ ê°ì²´ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš° softmaxëŠ” ì ì ˆí•˜ê²Œ ê°ì²´ë¥¼ ì•Œì•„ë‚´ì§€ ëª»í•˜ê¸° ë•Œë¬¸ì—, box ì•ˆì— ê° classê°€ ì¡´ì¬í•˜ëŠ” ì—¬ë¶€ë¥¼ í™•ì¸í•˜ëŠ” binary cross-entropyê°€ ë³´ë‹¤ ì ì ˆí•˜ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.   

$o$ (objectness)ëŠ” anchorì™€ bboxì˜ iouê°€ ê°€ì¥ í° anchorì˜ ê°’ì´ 1, ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš°ì˜ ê°’ì´ 0ì¸ $\left[N, N, 3, 1\right]$ì˜ tensorë¡œ ë§Œë“¤ì–´ì§„ë‹¤. $c$ (class label)ì€ one-encodingìœ¼ë¡œ $\left[N, N, 3, n \right]$ ($n$ : num_classes) ì˜ shapeë¥¼ ê°–ëŠ” tensorë¡œ ë§Œë“¤ì–´ì§„ë‹¤. 


# YOLOv4
## Model
<p align="center"><img src="https://github.com/em-1001/CSPDarknet53-SPP/blob/master/image/CSPDarknet53.png" height="55%" width="55%"></p>

ì „ì²´ì ì¸ êµ¬ì¡°ëŠ” YOLOv3ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ YOLOv4ëŠ” **CSPDarknet53+SPP**ë¥¼ ì‚¬ìš©í•œë‹¤. CSPDarknet53ì€ Darknet53ì— CSPNetì„ ì ìš©í•œ ê²ƒì´ë‹¤. CSPNetì€ ìœ„ ì‚¬ì§„ì˜ CSP Residual ë¶€ë¶„ê³¼ ê°™ì´ base layerì˜ feature mapì„ ë‘ ê°œë¡œ ë‚˜ëˆˆ ë’¤($X_0 \to X_0^{'}, X_0^{''}$) $X_0^{''}$ëŠ” Dense Layerì— í†µê³¼ ì‹œí‚¤ê³  $X_0^{'}$ëŠ” ê·¸ëŒ€ë¡œ ê°€ì ¸ì™€ì„œ ë§ˆì§€ë§‰ì— Dense Layerì˜ ì¶œë ¥ê°’ì¸ ($X_0^{''}, x_1, x_2, ...$)ì„ transition layerì— í†µê³¼ì‹œí‚¨ $X_T$ì™€ concatì‹œí‚¨ë‹¤. ì´í›„ concatëœ ê²°ê³¼ê°€ ë‹¤ìŒ transition layerë¥¼ í†µê³¼í•˜ë©´ì„œ $X_U$ê°€ ìƒì„±ëœë‹¤.

$$
\begin{aligned}
X_k &= W_K^{ * }[X_0^{''}, X_1, ..., X_{k-1}]\\  
X_T &= W_T^{ * }[X_0^{''}, X_1, ..., X_{k}]\\    
X_U &= W_U^{ * }[X_0^{'}, X_T]\\      
\end{aligned}$$  
</br>

$$
\begin{aligned}
W_k^{'} &= f(W_k, g_0^{''}, g_1, g_2, ..., g_{k-1})\\  
W_T^{'} &= f(W_T, g_0^{''}, g_1, g_2, ..., g_{k})\\  
W_U^{'} &= f(W_U, g_0^{'}, g_T)\\      
\end{aligned}$$

ì´ë ‡ê²Œ í•˜ë¯€ë¡œì¨ CSPDenseNetì€ DenseNetì˜ feature reuse íŠ¹ì„±ì„ í™œìš©í•˜ë©´ì„œ, gradient flowë¥¼ truncate($X_0 \to X_0^{'}, X_0^{''}$)í•˜ì—¬ ê³¼ë„í•œ ì–‘ì˜ gradient information ë³µì‚¬ë¥¼ ë°©ì§€í•  ìˆ˜ ìˆë‹¤. 


## Box Loss
ì¼ë°˜ì ìœ¼ë¡œ IoU-based lossëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ëœë‹¤. 

$$L = 1 - IoU + \mathcal{R}(B, B^{gt})$$

ì—¬ê¸°ì„œ $R(B, B^{gt})$ëŠ”  predicted box $B$ì™€ target box $B^{gt}$ì— ëŒ€í•œ penalty termì´ë‹¤.  
$1 - IoU$ë¡œë§Œ Lossë¥¼ êµ¬í•  ê²½ìš° boxê°€ ê²¹ì¹˜ì§€ ì•ŠëŠ” caseì— ëŒ€í•´ì„œ ì–´ëŠ ì •ë„ì˜ ì˜¤ì°¨ë¡œ êµì§‘í•©ì´ ìƒê¸°ì§€ ì•Šì€ ê²ƒì¸ì§€ ì•Œ ìˆ˜ ì—†ì–´ì„œ gradient vanishing ë¬¸ì œê°€ ë°œìƒí–ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ penalty termì„ ì¶”ê°€í•œ ê²ƒì´ë‹¤. 

### Generalized-IoU(GIoU)
Generalized-IoU(GIoU) ì˜ ê²½ìš° LossëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ëœë‹¤. 

$$L_{GIoU} = 1 - IoU + \frac{|C - B âˆª B^{gt}|}{|C|}$$

ì—¬ê¸°ì„œ $C$ëŠ” $B$ì™€ $B^{gt}$ë¥¼ ëª¨ë‘ í¬í•¨í•˜ëŠ” ìµœì†Œ í¬ê¸°ì˜ Boxë¥¼ ì˜ë¯¸í•œë‹¤. Generalized-IoUëŠ” ê²¹ì¹˜ì§€ ì•ŠëŠ” ë°•ìŠ¤ì— ëŒ€í•œ gradient vanishing ë¬¸ì œëŠ” ê°œì„ í–ˆì§€ë§Œ horizontalê³¼ verticalì— ëŒ€í•´ì„œ ì—ëŸ¬ê°€ í¬ë‹¤. ì´ëŠ” target boxì™€ ìˆ˜í‰, ìˆ˜ì§ì„ ì„ ì´ë£¨ëŠ” Anchor boxì— ëŒ€í•´ì„œëŠ” $|C - B âˆª B^{gt}|$ê°€ ë§¤ìš° ì‘ê±°ë‚˜ 0ì— ê°€ê¹Œì›Œì„œ IoUì™€ ë¹„ìŠ·í•˜ê²Œ ë™ì‘í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ë˜í•œ ê²¹ì¹˜ì§€ ì•ŠëŠ” boxì— ëŒ€í•´ì„œ ì¼ë‹¨ predicted boxì˜ í¬ê¸°ë¥¼ ë§¤ìš° í‚¤ìš°ê³  IoUë¥¼ ëŠ˜ë¦¬ëŠ” ë™ì‘ íŠ¹ì„± ë•Œë¬¸ì— ìˆ˜ë ´ ì†ë„ê°€ ëŠë¦¬ë‹¤. 

### Distance-IoU(DIoU)
GIoUê°€ ë©´ì  ê¸°ë°˜ì˜ penalty termì„ ë¶€ì—¬í–ˆë‹¤ë©´, DIoUëŠ” ê±°ë¦¬ ê¸°ë°˜ì˜ penalty termì„ ë¶€ì—¬í•œë‹¤. 
DIoUì˜ penalty termì€ ë‹¤ìŒê³¼ ê°™ë‹¤. 

$$\mathcal{R}_{DIoU} = \frac{\rho^2(b, b^{gt})}{c^2}$$

$\rho^2$ëŠ” Euclideanê±°ë¦¬ì´ë©° $c$ëŠ” $B$ì™€ $B^{gt}$ë¥¼ í¬í•¨í•˜ëŠ” ê°€ì¥ ì‘ì€ Boxì˜ ëŒ€ê°ì„  ê±°ë¦¬ì´ë‹¤. 

<p align="center"><img src="https://github.com/em-1001/YOLOv3/blob/master/image/diou.png" height="25%" width="25%"></p>

DIoU LossëŠ” ë‘ ê°œì˜ boxê°€ ì™„ë²½íˆ ì¼ì¹˜í•˜ë©´ 0, ë§¤ìš° ë©€ì–´ì§€ë©´ $L_{GIoU} = L_{DIoU} \to 2$ê°€ ëœë‹¤. ì´ëŠ” IoUê°€ 0ì´ ë˜ê³ , penalty termì´ 1ì— ê°€ê¹ê²Œ ë˜ê¸° ë•Œë¬¸ì´ë‹¤. Distance-IoUëŠ” ë‘ boxì˜ ì¤‘ì‹¬ ê±°ë¦¬ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì¤„ì´ê¸° ë•Œë¬¸ì— GIoUì— ë¹„í•´ ìˆ˜ë ´ì´ ë¹ ë¥´ê³ , ê±°ë¦¬ê¸°ë°˜ì´ë¯€ë¡œ ìˆ˜í‰, ìˆ˜ì§ë°©í–¥ì—ì„œ ë˜í•œ ìˆ˜ë ´ì´ ë¹ ë¥´ë‹¤. 

#### DIoU-NMS
DIoUë¥¼ NMS(Non-Maximum Suppression)ì—ë„ ì ìš©í•  ìˆ˜ ìˆë‹¤. ì¼ë°˜ì ì¸ NMSì˜ ê²½ìš° ì´ë¯¸ì§€ì—ì„œ ê°™ì€ classì¸ ë‘ ë¬¼ì²´ê°€ ê²¹ì³ìˆëŠ” Occlusion(ê°€ë¦¼)ì´ ë°œìƒí•œ ê²½ìš° ì˜¬ë°”ë¥¸ ë°•ìŠ¤ê°€ ì‚­ì œë˜ëŠ” ë¬¸ì œê°€ ë°œìƒí•˜ëŠ”ë°, DIoUë¥¼ ì ‘ëª©í•  ê²½ìš° ë‘ ë°•ìŠ¤ì˜ ì¤‘ì‹¬ì  ê±°ë¦¬ë„ ê³ ë ¤í•˜ê¸° ë•Œë¬¸ì— target boxë¼ë¦¬ ê²¹ì³ì§„ ê²½ìš°ì—ë„ robustí•˜ê²Œ ë™ì‘í•  ìˆ˜ ìˆë‹¤. 

$$
s_i =
\begin{cases}
s_ i, & IoU - \mathcal{R}_ {DIoU}(\mathcal{M}, B_i) < \epsilon\\
0, & IoU - \mathcal{R}_{DIoU}(\mathcal{M}, B_i) \ge \epsilon
\end{cases}
$$

ê°€ì¥ ë†’ì€ Confidence scoreë¥¼ ê°–ëŠ” $\mathcal{M}$ì— ëŒ€í•´ IoUì™€ DIoUì˜ distance penaltyë¥¼ ë™ì‹œì— ê³ ë ¤í•˜ì—¬ IoUê°€ ë§¤ìš° í¬ë”ë¼ë„ ì¤‘ì‹¬ì  ì‚¬ì´ì˜ ê±°ë¦¬ê°€ ë©€ë©´ ë‹¤ë¥¸ ê°ì²´ë¥¼ íƒì§€í•œ ê²ƒì¼ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ ìœ„ì™€ ê°™ì´ ì¼ì • ì„ê³„ì¹˜ $\epsilon$ ë³´ë‹¤ ì‘ìœ¼ë©´ ì—†ì• ì§€ ì•Šê³  ë³´ì¡´í•œë‹¤. 


### Complete-IoU(CIoU)
DIoU, CIoUë¥¼ ì œì•ˆí•œ ë…¼ë¬¸ì—ì„œ ë§í•˜ëŠ” ì„±ê³µì ì¸ Bounding Box Regressionì„ ìœ„í•œ 3ê°€ì§€ ì¡°ê±´ì€ overlap area, central point
distance, aspect ratioì´ë‹¤. ì´ ì¤‘ overlap area, central pointëŠ” DIoUì—ì„œ ì´ë¯¸ ê³ ë ¤í–ˆê³  ì—¬ê¸°ì— aspect ratioë¥¼ ê³ ë ¤í•œ penalty termì„ ì¶”ê°€í•œ ê²ƒì´ CIoUì´ë‹¤. CIoU penalty termëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤. 

$$\mathcal{R}_{CIoU} = \frac{\rho^2(b, b^{gt})}{c^2} + \alpha v$$

$$v = \frac{4}{Ï€^2}(\arctan{\frac{w^{gt}}{h^{gt}}} - \arctan{\frac{w}{h}})^2$$

$$\alpha = \frac{v}{(1 - IoU) + v}$$

$v$ì˜ ê²½ìš° bboxëŠ” ì§ì‚¬ê°í˜•ì´ê³  $\arctan{\frac{w}{h}} = \theta$ì´ë¯€ë¡œ $\theta$ì˜ ì°¨ì´ë¥¼ í†µí•´ aspect ratioë¥¼ êµ¬í•˜ê²Œ ëœë‹¤. ì´ë•Œ $v$ì— $\frac{2}{Ï€}$ê°€ ê³±í•´ì§€ëŠ” ì´ìœ ëŠ” $\arctan$ í•¨ìˆ˜ì˜ ìµœëŒ€ì¹˜ê°€ $\frac{2}{Ï€}$ ì´ë¯€ë¡œ scaleì„ ì¡°ì •í•´ì£¼ê¸° ìœ„í•´ì„œì´ë‹¤. 

$\alpha$ëŠ” trade-off íŒŒë¼ë¯¸í„°ë¡œ IoUê°€ í° boxì— ëŒ€í•´ ë” í° penaltyë¥¼ ì£¼ê²Œ ëœë‹¤. 

CIoUì— ëŒ€í•´ ìµœì í™”ë¥¼ ìˆ˜í–‰í•˜ë©´ ì•„ë˜ì™€ ê°™ì€ ê¸°ìš¸ê¸°ë¥¼ ì–»ê²Œ ëœë‹¤. ì´ë•Œ, $w, h$ëŠ” ëª¨ë‘ 0ê³¼ 1ì‚¬ì´ë¡œ ê°’ì´ ì‘ì•„ gradient explosionì„ ìœ ë°œí•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì‹¤ì œ êµ¬í˜„ ì‹œì—ëŠ” $\frac{1}{w^2 + h^2} = 1$ë¡œ ì„¤ì •í•œë‹¤. 

$$\frac{\partial v}{\partial w} = \frac{8}{Ï€^2}(\arctan{\frac{w^{gt}}{h^{gt}}} - \arctan{\frac{w}{h}}) \times \frac{h}{w^2 + h^2}$$ 

$$\frac{\partial v}{\partial h} = -\frac{8}{Ï€^2}(\arctan{\frac{w^{gt}}{h^{gt}}} - \arctan{\frac{w}{h}}) \times \frac{w}{w^2 + h^2}$$ 

## Cosine Annealing
Cosine annealingì€ í•™ìŠµìœ¨ì˜ ìµœëŒ€ê°’ê³¼ ìµœì†Œê°’ì„ ì •í•´ì„œ ê·¸ ë²”ìœ„ì˜ í•™ìŠµìœ¨ì„ ì½”ì‹¸ì¸ í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ìŠ¤ì¼€ì¥´ë§í•˜ëŠ” ë°©ë²•ì´ë‹¤. Cosine anneaingì˜ ì´ì ì€ ìµœëŒ€ê°’ê³¼ ìµœì†Œê°’ ì‚¬ì´ì—ì„œ ì½”ì‹¸ì¸ í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ê¸‰ê²©íˆ ì¦ê°€ì‹œì¼°ë‹¤ê°€ ê¸‰ê²©íˆ ê°ì†Œì‹œí‚¤ ë•Œë¬¸ì— ëª¨ë¸ì˜ ë§¤ë‹ˆí´ë“œ ê³µê°„ì˜ ì•ˆì¥(saddle point)ë¥¼ ë¹ ë¥´ê²Œ ë²—ì–´ë‚  ìˆ˜ ìˆìœ¼ë©°, í•™ìŠµ ì¤‘ê°„ì— ìƒê¸°ëŠ” ì •ì²´ êµ¬ê°„ë“¤ ë˜í•œ ë¹ ë¥´ê²Œ ë²—ì–´ë‚  ìˆ˜ ìˆë„ë¡ í•œë‹¤.

$$\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})\left(1 + \cos{\left(\frac{T_{cur}}{T_{\max}}\pi\right)} \right), \ T_{cur} \neq (2k+1)T_{\max}$$

$\eta_{\min}$ : min learning rate    
$\eta_{\max}$ : max learning rate    
$T_{\max}$ : period


https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html   
code : https://github.com/pytorch/pytorch/blob/v1.1.0/torch/optim/lr_scheduler.py#L222

```py
import torch
import torch.optim as optim

class CosineAnnealingLRWithWarmup:
    def __init__(self, optimizer, warmup_epochs, max_epochs, eta_min=0):
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.max_epochs = max_epochs
        self.eta_min = eta_min
        self.epoch = 0

    def get_lr(self):
        if self.epoch < self.warmup_epochs:
            return self.epoch / self.warmup_epochs
        else:
            return self.eta_min + 0.5 * (1 + torch.cos((self.epoch - self.warmup_epochs) / (self.max_epochs - self.warmup_epochs) * math.pi))

    def step(self):
        self.epoch += 1
        lr = self.get_lr()
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
```

# Performance
<img src="https://github.com/em-1001/YOLOv3/blob/master/image/cat0_1.png">&#160;&#160;&#160;&#160;<img src="https://github.com/em-1001/YOLOv3/blob/master/image/cat1_1.png">     

### configuration  
```ini
DATASET = PASCAL_VOC
ANCHORS = [
    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],
    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],
    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],
] 

# YOLOv3  

BATCH_SIZE = 32
OPTIMIZER = Adam
NUM_EPOCHS = 70
CONF_THRESHOLD = 0.05
MAP_IOU_THRESH = 0.5
NMS_IOU_THRESH = 0.45
WEIGHT_DECAY = 1e-4
LEARNING_RATE = 1e-5


# YOLOv4

BATCH_SIZE = 32
OPTIMIZER = Adam
NUM_EPOCHS = 70
CONF_THRESHOLD = 0.05
MAP_IOU_THRESH = 0.5
NMS_IOU_THRESH = 0.45
WEIGHT_DECAY = 1e-4

# 0 ~ 30 epoch                # Cosine Annealing                            

LEARNING_RATE = 0.0001        LEARNING_RATE = 0.0001        
                              T_max=80
# 30 ~ 50 epoch               

LEARNING_RATE = 0.00005       

# 50 ~  epoch                

LEARNING_RATE = 0.00001      

```
CA : https://github.com/csm-kr/YOLOv4_pytorch/blob/master/main.py  
CA ì‚¬ìš©ë²• : https://wikidocs.net/180475  
```
# mAP per 10 epochs
yolov3 : 0 -> 5 -> 10 -> 15 -> 15 -> 18 ...
CSP : 3 -> 14 -> 28 -> 32 -> 36 -> 43.6 -> 42.5
CSP+CIoU : 3 -> 14 -> 29 -> 35.7 -> 38.7 -> 41.1 -> 45.7
CSP+CIoU+CA : 2.9 -> 
```

### NMS(Non-maximum Suppression)
|Detection|320 x 320|416 x 416|512 x 512|
|--|--|--|--|
|YOLOv3|?|31.7|?|
|YOLOv3 + CSP|?|42.5|?|
|YOLOv3 + CSP + CIoU|?|45.7|?|
|YOLOv3 + CSP + CIoU + CA|?|?|?|
|YOLOv3 + CSP + CIoU + CA + M|?|?|?|
|YOLOv3 + CSP + CIoU + CA + M + IT|?|?|?|

### DIoU-NMS
|Detection|320 x 320|416 x 416|512 x 512|
|--|--|--|--|
|YOLOv3 + MSE DIoU-NMS|?|?|?|
|YOLOv3 + CSP + CIoU + CA DIoU-NMS|?|?|?|

https://csm-kr.tistory.com/62

The model was evaluated with confidence 0.2 and IOU threshold 0.45 using NMS.

### Pretrained Weights
YOLOv3 + CSP : https://www.kaggle.com/datasets/sj2129tommy/csp-70epoch   
YOLOv3 + CSP + CIoU : https://www.kaggle.com/datasets/sj2129tommy/csp-ciou-70epoch  

# Reference
## Web Link 
One-stage object detection : https://machinethink.net/blog/object-detection/   
DIoU, CIoU : https://hongl.tistory.com/215  
YOLOv3 : https://herbwood.tistory.com/21    
&#160;&#160;&#160;&#160;&#160;&#160;&#160;ã€€ã€€ https://csm-kr.tistory.com/11   
&#160;&#160;&#160;&#160;&#160;&#160;&#160;ã€€ã€€ https://towardsdatascience.com/dive-really-deep-into-yolo-v3-a-beginners-guide-9e3d2666280e  
YOLOv4 : https://wikidocs.net/181720   
ã€€ã€€ã€€ ã€€https://csm-kr.tistory.com/62  
Residual block : https://daeun-computer-uneasy.tistory.com/28  
ã€€ã€€ã€€ã€€ã€€ã€€ã€€https://techblog-history-younghunjo1.tistory.com/279     
NMS : https://wikidocs.net/142645     
mAP : https://ctkim.tistory.com/entry/mAPMean-Average-Precision-%EC%A0%95%EB%A6%AC   
BottleNeck : https://velog.io/@lighthouse97/CNN%EC%9D%98-Bottleneck%EC%97%90-%EB%8C%80%ED%95%9C-%EC%9D%B4%ED%95%B4   
Cosine Annealing : https://ai4nlp.tistory.com/16  


## Paper
YOLOv2 : https://arxiv.org/pdf/1612.08242.pdf      
YOLOv3 : https://arxiv.org/pdf/1804.02767.pdf  
YOLOv4 : https://arxiv.org/pdf/2004.10934.pdf  
DIoU, CIoU : https://arxiv.org/pdf/1911.08287.pdf    
DenseNet : https://arxiv.org/pdf/1608.06993.pdf    
CSPNet : https://arxiv.org/pdf/1911.11929.pdf    
SPPNet : https://arxiv.org/pdf/1406.4729.pdf    
SGDR : https://arxiv.org/pdf/1608.03983v5.pdf  
